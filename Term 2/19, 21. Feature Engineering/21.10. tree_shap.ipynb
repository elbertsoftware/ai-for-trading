{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Shap\n",
    "\n",
    "To get a sense of the Shap library that we'll be using, let's implement the simple version of the Tree Shap algorithm.  This is based on Scott Lundberg's paper [Consistent Individualized Feature Attribution for Tree\n",
    "Ensembles](https://arxiv.org/pdf/1802.03888.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and re-using a single tree model\n",
    "\n",
    "You may notice that calculating Shap values for every feature, and for every individual data point, is very computationally expensive.  For example, we would be training multiple models just to calculate the importance of one feature.\n",
    "\n",
    "With decision trees, we can actually train a decision tree on all the features, and then re-use that single tree to calculate shapley values using subsets of that single tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./tree_shap_images/tree_shap_img_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some intuition, let’s look at a model that has just two features.  The tree splits on feature one first, and then on feature 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tree_shap_img_02.png](./tree_shap_images/tree_shap_img_02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to see how a model would perform if it only used the first feature, we could look at the subtree that consists of the top three nodes of this tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tree_shap_img_03.png](./tree_shap_images/tree_shap_img_03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, if we wanted to see how a model would make predictions if it used only feature 2, we could look at the subtree containing the bottom three nodes, starting at the node that splits on feature 2.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tree_shap_img_04.png](./tree_shap_images/tree_shap_img_04.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another example with 3 features\n",
    "Now let’s look at a tree that is trained on three features.  Let’s say it splits on feature 1, then on feature 2, then on feature 3.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tree_shap_img_04.png](./tree_shap_images/tree_shap_img_05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we simulate the prediction of a tree that was only trained on features 1 and 3, but not on feature 2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tree_shap_img_04.png](./tree_shap_images/tree_shap_img_06.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, if we didn’t split on feature 2, that means that we would include the training samples in both the left and right sub-tree of that node when making a prediction.  This is how we can simulate that the tree never split on feature 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tree_shap_img_04.png](./tree_shap_images/tree_shap_img_07.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s also think about how we handle the predictions when we do split on the feature.  If we split on feature 3, and the particular data point we’re making a prediction for ends up in the left child node, then we can use the prediction based on training samples in the left sub-tree, and ignore the training samples in the right sub-tree.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tree_shap_img_04.png](./tree_shap_images/tree_shap_img_08.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll walk through the algorithm to do this, and then you’ll get to practice this yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm\n",
    "\n",
    "Here’s the algorithm used to calculate the prediction of a tree, given a subset of features.  You can check out the paper [Consistent Individualized Feature Attribution for Tree Ensembles](https://arxiv.org/pdf/1802.03888.pdf), page 4 algorithm 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tree_shap_img_04.png](./tree_shap_images/tree_shap_img_09.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, $G$ is a function that gets called recursively to walk down the tree starting at the root node.  $w$ is the weight given to the predictions of each node.  $v$ is the prediction of a leaf node.  $r_{a_j}$ and $r_{b_j}$ are the number of data points in the left and right child nodes of node $j$.  $r_j$ is the number of data points in node $j$.\n",
    "\n",
    "We can use this to walk through a decision tree that is trained on all features, and calculate the prediction of a tree that would have been created from a subset of the features.\n",
    "\n",
    "Let's look at specific parts of this algorithm in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### leaf nodes\n",
    "Let's look at the the line that handles leaf nodes.  It takes the prediction of that leaf node and multiplies it by some weight.  The weight is determined by the proportion of training data points that end up reaching that leaf node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tree_shap_img_04.png](./tree_shap_images/tree_shap_img_10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ignoring a feature\n",
    "Next, let’s look at the case when the feature that’s used at a node is not within the subset of features that we want to split on.  In other words, we want to pretend that we didn’t train the model on this feature.  In that case, in order to pretend that we’re not splitting on that feature, we take the sum of the weighted predictions from both its left and right subtree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tree_shap_img_04.png](./tree_shap_images/tree_shap_img_11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### including a feature\n",
    "\n",
    "Finally, for cases when the feature at that node is within the subset of features that we want to use, then we can follow just the left subtree or just the right subtree, whichever path that the input data gets assigned to by the split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tree_shap_img_04.png](./tree_shap_images/tree_shap_img_12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement it in code!\n",
    "You’ll get to practice this algorithm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.14.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/1e/116ad560de97694e2d0c1843a7a0075cc9f49e922454d32f49a80eb6f1f2/numpy-1.14.5-cp36-cp36m-manylinux1_x86_64.whl (12.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 12.2MB 2.8MB/s eta 0:00:01   10% |███▌                            | 1.3MB 21.7MB/s eta 0:00:01    30% |█████████▋                      | 3.7MB 24.5MB/s eta 0:00:01    39% |████████████▋                   | 4.8MB 22.9MB/s eta 0:00:01    57% |██████████████████▍             | 7.0MB 23.4MB/s eta 0:00:01    85% |███████████████████████████▍    | 10.4MB 24.2MB/s eta 0:00:01\n",
      "\u001b[31mtensorflow 1.3.0 requires tensorflow-tensorboard<0.2.0,>=0.1.0, which is not installed.\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Found existing installation: numpy 1.12.1\n",
      "    Uninstalling numpy-1.12.1:\n",
      "      Successfully uninstalled numpy-1.12.1\n",
      "Successfully installed numpy-1.14.5\n",
      "Requirement already satisfied: scikit-learn==0.19.1 in /opt/conda/lib/python3.6/site-packages (0.19.1)\n",
      "Collecting graphviz==0.9\n",
      "  Downloading https://files.pythonhosted.org/packages/47/87/313cd4ea4f75472826acb74c57f94fc83e04ba93e4ccf35656f6b7f502e2/graphviz-0.9-py2.py3-none-any.whl\n",
      "Installing collected packages: graphviz\n",
      "Successfully installed graphviz-0.9\n",
      "Collecting shap==0.25.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/85/196abf4c178446e077237e4c649e93637393dac24ab7f760143c4ef1548e/shap-0.25.2.tar.gz (197kB)\n",
      "\u001b[K    100% |████████████████████████████████| 204kB 8.0MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from shap==0.25.2) (1.14.5)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from shap==0.25.2) (1.2.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.6/site-packages (from shap==0.25.2) (0.19.1)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.6/site-packages (from shap==0.25.2) (2.1.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from shap==0.25.2) (0.23.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from shap==0.25.2) (4.11.2)\n",
      "Requirement already satisfied: ipython in /opt/conda/lib/python3.6/site-packages (from shap==0.25.2) (6.5.0)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib->shap==0.25.2) (1.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.0 in /opt/conda/lib/python3.6/site-packages (from matplotlib->shap==0.25.2) (2.6.1)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.6/site-packages (from matplotlib->shap==0.25.2) (2017.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages/cycler-0.10.0-py3.6.egg (from matplotlib->shap==0.25.2) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->shap==0.25.2) (2.2.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.6/site-packages (from ipython->shap==0.25.2) (4.3.2)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.6/site-packages (from ipython->shap==0.25.2) (0.7.4)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /opt/conda/lib/python3.6/site-packages (from ipython->shap==0.25.2) (0.8.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.6/site-packages (from ipython->shap==0.25.2) (38.4.0)\n",
      "Requirement already satisfied: jedi>=0.10 in /opt/conda/lib/python3.6/site-packages (from ipython->shap==0.25.2) (0.10.2)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.6/site-packages (from ipython->shap==0.25.2) (0.1.0)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /opt/conda/lib/python3.6/site-packages (from ipython->shap==0.25.2) (4.3.1)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.6/site-packages (from ipython->shap==0.25.2) (4.0.11)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.15 in /opt/conda/lib/python3.6/site-packages (from ipython->shap==0.25.2) (1.0.15)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.6/site-packages (from ipython->shap==0.25.2) (2.2.0)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.6/site-packages (from traitlets>=4.2->ipython->shap==0.25.2) (0.2.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.6/site-packages (from pexpect; sys_platform != \"win32\"->ipython->shap==0.25.2) (0.5.2)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.6/site-packages (from prompt-toolkit<2.0.0,>=1.0.15->ipython->shap==0.25.2) (0.1.7)\n",
      "Building wheels for collected packages: shap\n",
      "  Running setup.py bdist_wheel for shap ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/3f/cb/58/2482f534eb1c1f9ac93653887aee5b6ea83718ef041c1310bd\n",
      "Successfully built shap\n",
      "Installing collected packages: shap\n",
      "Successfully installed shap-0.25.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install numpy==1.14.5\n",
    "!{sys.executable} -m pip install scikit-learn==0.19.1\n",
    "!{sys.executable} -m pip install graphviz==0.9\n",
    "!{sys.executable} -m pip install shap==0.25.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.ensemble\n",
    "import shap\n",
    "import numpy as np\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate sample data\n",
    "\n",
    "Feature 0 and feature 1 form the AND operator, and feature 2 does not contribute to the prediction of the label, because it's always zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AND case (features 0 and 1)\n",
    "N = 100\n",
    "M = 3\n",
    "X = np.zeros((N,M))\n",
    "X.shape\n",
    "y = np.zeros(N)\n",
    "X[:1 * N//4, 1] = 1\n",
    "X[:N//2, 0] = 1\n",
    "X[N//2:3 * N//4, 1] = 1\n",
    "y[:1 * N//4] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train a decision tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"310pt\" height=\"261pt\"\n",
       " viewBox=\"0.00 0.00 310.00 261.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 257)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-257 306,-257 306,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.250980\" stroke=\"black\" d=\"M166.5,-253C166.5,-253 71.5,-253 71.5,-253 65.5,-253 59.5,-247 59.5,-241 59.5,-241 59.5,-201 59.5,-201 59.5,-195 65.5,-189 71.5,-189 71.5,-189 166.5,-189 166.5,-189 172.5,-189 178.5,-195 178.5,-201 178.5,-201 178.5,-241 178.5,-241 178.5,-247 172.5,-253 166.5,-253\"/>\n",
       "<text text-anchor=\"start\" x=\"87.5\" y=\"-238.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">X</text>\n",
       "<text text-anchor=\"start\" x=\"98.5\" y=\"-238.8\" font-family=\"Helvetica,sans-Serif\" baseline-shift=\"sub\" font-size=\"14.00\">1</text>\n",
       "<text text-anchor=\"start\" x=\"106.5\" y=\"-238.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"> ≤ 0.5</text>\n",
       "<text text-anchor=\"start\" x=\"74\" y=\"-224.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">mse = 0.188</text>\n",
       "<text text-anchor=\"start\" x=\"67.5\" y=\"-210.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 100</text>\n",
       "<text text-anchor=\"start\" x=\"73.5\" y=\"-196.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 0.25</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M98,-147.5C98,-147.5 12,-147.5 12,-147.5 6,-147.5 0,-141.5 0,-135.5 0,-135.5 0,-106.5 0,-106.5 0,-100.5 6,-94.5 12,-94.5 12,-94.5 98,-94.5 98,-94.5 104,-94.5 110,-100.5 110,-106.5 110,-106.5 110,-135.5 110,-135.5 110,-141.5 104,-147.5 98,-147.5\"/>\n",
       "<text text-anchor=\"start\" x=\"19\" y=\"-132.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">mse = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-117.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 50</text>\n",
       "<text text-anchor=\"start\" x=\"14\" y=\"-102.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 0.0</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M98.75,-188.992C91.9408,-178.566 84.3073,-166.877 77.3462,-156.218\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"80.0883,-154.015 71.6899,-147.556 74.2274,-157.843 80.0883,-154.015\"/>\n",
       "<text text-anchor=\"middle\" x=\"66.5549\" y=\"-168.323\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.501961\" stroke=\"black\" d=\"M226,-153C226,-153 140,-153 140,-153 134,-153 128,-147 128,-141 128,-141 128,-101 128,-101 128,-95 134,-89 140,-89 140,-89 226,-89 226,-89 232,-89 238,-95 238,-101 238,-101 238,-141 238,-141 238,-147 232,-153 226,-153\"/>\n",
       "<text text-anchor=\"start\" x=\"151.5\" y=\"-138.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">X</text>\n",
       "<text text-anchor=\"start\" x=\"162.5\" y=\"-138.8\" font-family=\"Helvetica,sans-Serif\" baseline-shift=\"sub\" font-size=\"14.00\">0</text>\n",
       "<text text-anchor=\"start\" x=\"170.5\" y=\"-138.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"> ≤ 0.5</text>\n",
       "<text text-anchor=\"start\" x=\"142.5\" y=\"-124.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">mse = 0.25</text>\n",
       "<text text-anchor=\"start\" x=\"136\" y=\"-110.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 50</text>\n",
       "<text text-anchor=\"start\" x=\"142\" y=\"-96.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 0.5</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M139.25,-188.992C144.911,-180.323 151.143,-170.782 157.083,-161.685\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"160.125,-163.429 162.662,-153.142 154.264,-159.601 160.125,-163.429\"/>\n",
       "<text text-anchor=\"middle\" x=\"167.797\" y=\"-173.909\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\"><title>3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M162,-53C162,-53 76,-53 76,-53 70,-53 64,-47 64,-41 64,-41 64,-12 64,-12 64,-6 70,-0 76,-0 76,-0 162,-0 162,-0 168,-0 174,-6 174,-12 174,-12 174,-41 174,-41 174,-47 168,-53 162,-53\"/>\n",
       "<text text-anchor=\"start\" x=\"83\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">mse = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"72\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 25</text>\n",
       "<text text-anchor=\"start\" x=\"78\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 0.0</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M161.511,-88.9415C155.402,-80.1118 148.717,-70.451 142.496,-61.4586\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"145.274,-59.3229 136.706,-53.0908 139.518,-63.3058 145.274,-59.3229\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\"><title>4</title>\n",
       "<path fill=\"#e58139\" stroke=\"black\" d=\"M290,-53C290,-53 204,-53 204,-53 198,-53 192,-47 192,-41 192,-41 192,-12 192,-12 192,-6 198,-0 204,-0 204,-0 290,-0 290,-0 296,-0 302,-6 302,-12 302,-12 302,-41 302,-41 302,-47 296,-53 290,-53\"/>\n",
       "<text text-anchor=\"start\" x=\"211\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">mse = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"200\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 25</text>\n",
       "<text text-anchor=\"start\" x=\"206\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 1.0</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\"><title>2&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M204.489,-88.9415C210.598,-80.1118 217.283,-70.451 223.504,-61.4586\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"226.482,-63.3058 229.294,-53.0908 220.726,-59.3229 226.482,-63.3058\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x7f7b47ea2ef0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model = sklearn.tree.DecisionTreeRegressor(random_state=0)\n",
    "model.fit(X, y)\n",
    "\n",
    "# draw model\n",
    "dot_data = sklearn.tree.export_graphviz(model, out_file=None, filled=True, rounded=True, special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tree attributes\n",
    "\n",
    "[sklearn.tree.tree._tree](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_tree.pyx)\n",
    "\n",
    "```\n",
    "The binary tree is represented as a number of parallel arrays. The i-th\n",
    "    element of each array holds information about the node `i`. Node 0 is the\n",
    "    tree's root.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree0 = model.tree_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  left and right child nodes\n",
    "```\n",
    "children_left : array of int, shape [node_count]\n",
    "        children_left[i] holds the node id of the left child of node i.\n",
    "        For leaves, children_left[i] == TREE_LEAF. Otherwise,\n",
    "        children_left[i] > i. This child handles the case where\n",
    "        X[:, feature[i]] <= threshold[i].\n",
    "    children_right : array of int, shape [node_count]\n",
    "        children_right[i] holds the node id of the right child of node i.\n",
    "        For leaves, children_right[i] == TREE_LEAF. Otherwise,\n",
    "        children_right[i] > i. This child handles the case where\n",
    "        X[:, feature[i]] > threshold[i].\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree0.children_left: [ 1 -1  3 -1 -1]\n",
      "tree0.children_right: [ 2 -1  4 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "print(f\"tree0.children_left: {tree0.children_left}\")\n",
    "print(f\"tree0.children_right: {tree0.children_right}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### features\n",
    "```\n",
    "    feature : array of int, shape [node_count]\n",
    "        feature[i] holds the feature to split on, for the internal node i.\n",
    "    threshold : array of double, shape [node_count]\n",
    "        threshold[i] holds the threshold for the internal node i.\n",
    "    value : array of double, shape [node_count, n_outputs, max_n_classes]\n",
    "        Contains the constant prediction value of each node.\n",
    "    impurity : array of double, shape [node_count]\n",
    "        impurity[i] holds the impurity (i.e., the value of the splitting\n",
    "        criterion) at node i.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree0.feature: [ 1 -2  0 -2 -2]\n"
     ]
    }
   ],
   "source": [
    "print(f\"tree0.feature: {tree0.feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Node 0, feature 1 is used to split the data . For Node 2, feature 0 is used for splitting.  For the other nodes (1, 3, 4), there are no features used for splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree0.threshold: [ 0.5 -2.   0.5 -2.  -2. ]\n"
     ]
    }
   ],
   "source": [
    "print(f\"tree0.threshold: {tree0.threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The threshold divides the data points using the chosen feature.  <= 0.5 and the data go to the left child; > .5 and the data go in the right child.  The -2 is for nodes that don't split on any feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree0.value : \n",
      "[[[0.25]]\n",
      "\n",
      " [[0.  ]]\n",
      "\n",
      " [[0.5 ]]\n",
      "\n",
      " [[0.  ]]\n",
      "\n",
      " [[1.  ]]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"tree0.value : \\n{tree0.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`value` is the average prediction for each node.  Node 0 predicts 0.25 on average.  Node 2 predicts 0.5 on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### node samples\n",
    "```\n",
    "n_node_samples : array of int, shape [node_count]\n",
    "        n_node_samples[i] holds the number of training samples reaching node i.\n",
    "\n",
    "weighted_n_node_samples : array of int, shape [node_count]\n",
    "        weighted_n_node_samples[i] holds the weighted number of training samples\n",
    "        reaching node i.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree0.n_node_samples : [100  50  50  25  25]\n",
      "tree0.weighted_n_node_samples : [100.  50.  50.  25.  25.]\n"
     ]
    }
   ],
   "source": [
    "print(f\"tree0.n_node_samples : {tree0.n_node_samples}\")\n",
    "print(f\"tree0.weighted_n_node_samples : {tree0.weighted_n_node_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The n_node_samples counts how many data points from the parent gets passed to each child node.  `weighted_n_node_samples` is the same in this case, because it's a single decision tree.  If this were a tree within a random forest, 2/3 of the training data may be sampled and used to train a tree.  The `weighted_n_node_samples` would then be re-scaled to equal the total sample size.  We can use either in the calculations we'll do below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### quiz\n",
    "What proportion of samples went to the left child and right child of the root node?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proportion of samples in left child of root node 0.5\n",
      "proportion of samples in left child of root node 0.5\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "proportion_in_left_child = tree0.weighted_n_node_samples[1] / tree0.weighted_n_node_samples[0]\n",
    "proportion_in_right_child = tree0.weighted_n_node_samples[2] / tree0.weighted_n_node_samples[0]\n",
    "print(f\"proportion of samples in left child of root node {proportion_in_left_child}\")\n",
    "print(f\"proportion of samples in left child of root node {proportion_in_right_child}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wrap with Tree class\n",
    "\n",
    "To make the tree object easier to work with, we'll wrap it inside our custom Tree class.  Please complete the functions within the Tree class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_NODE = -1\n",
    "NO_FEATURE = -2\n",
    "\n",
    "class Tree:\n",
    "    \n",
    "    def __init__(self, tree):\n",
    "        if str(type(tree)).endswith(\"'sklearn.tree._tree.Tree'>\"):\n",
    "            self.weight = 1 \n",
    "            self.children_left = tree.children_left\n",
    "            self.children_right = tree.children_right\n",
    "            self.features = tree.feature\n",
    "            self.thresholds = tree.threshold\n",
    "            self.values = tree.value[:,0,0] # tree.value is n by 1 by 1; get the n prediction values (there are n nodes in tree)\n",
    "            self.n_node_samples = tree.n_node_samples # actual number of nodes\n",
    "            self.node_sample_weight = tree.weighted_n_node_samples #rescaled number of nodes\n",
    "\n",
    "    def is_internal(self, i):\n",
    "        return (self.children_left[i] != NO_NODE or self.children_right[i] != NO_NODE)\n",
    "\n",
    "    def is_leaf(self, i):\n",
    "        # TODO\n",
    "        return self.children_left[i] == NO_NODE and self.children_right[i] == NO_NODE\n",
    "          \n",
    "    def left_child(self, i):\n",
    "        return self.children_left[i]\n",
    "\n",
    "    def right_child(self, i):\n",
    "        # TODO\n",
    "        return self.children_right[i]\n",
    "    \n",
    "    def proportion_of_samples_in_left_child(self,i):\n",
    "        # TODO\n",
    "        return self.n_node_samples[self.left_child(i)] / self.n_node_samples[i]\n",
    "        \n",
    "    def proportion_of_samples_in_right_child(self,i):\n",
    "        # TODO\n",
    "        return self.n_node_samples[self.right_child(i)] / self.n_node_samples[i]\n",
    "        \n",
    "    def node_prediction(self,i):\n",
    "        return self.values[i]\n",
    "\n",
    "    def feature_that_split_node_i(self,i):\n",
    "        return self.features[i]\n",
    "\n",
    "    def threshold_at_node_i(self,i):\n",
    "        return self.thresholds[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_wrap = Tree(tree0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate the prediction of a tree model, given a subset of features\n",
    "\n",
    "We'll implement algorithm 1 of Scott Lundberg's paper.  This is a way to use a single trained tree to estimate predictions of other trees that would be trained on a subset of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_given_S(tree, S, x):\n",
    "    \"\"\"\n",
    "    tree: the custom Tree class\n",
    "    S: set of integers reprenting features that are used to train the model.\n",
    "    x: sample observation on which to calculate the prediction of the model.\n",
    "    \"\"\"\n",
    "    # the root node is at index 0 in the list\n",
    "    starting_node = 0\n",
    "    \n",
    "    # when starting at the root node, the weight assigned is 1 (100%).\n",
    "    starting_weight = 1\n",
    "\n",
    "    def traverse_tree(node_i, weight):\n",
    "        \"\"\"\n",
    "        nested function that will be called recursively\n",
    "        \"\"\"\n",
    "        if tree.is_leaf(node_i):\n",
    "            # TODO: multiply the weight times the node prediction\n",
    "            return weight * tree.node_prediction(node_i)\n",
    "            \n",
    "        else: # is internal node\n",
    "            feature_index = tree.feature_that_split_node_i(node_i)\n",
    "            feature_value = x[feature_index]\n",
    "            threshold = tree.threshold_at_node_i(node_i)\n",
    "            left_child = tree.left_child(node_i)\n",
    "            right_child = tree.right_child(node_i)\n",
    "            \n",
    "            if feature_index in S:\n",
    "                if feature_value <= threshold:\n",
    "                    # TODO: recursively traverse the left subtree\n",
    "                    return traverse_tree(left_child, weight)\n",
    "                    \n",
    "                else:\n",
    "                    # TODO: recursively traverse the right subtree\n",
    "                    return traverse_tree(right_child, weight)\n",
    "                    \n",
    "            else: #feature is not in subset S\n",
    "                # TODO: traverse the left sub-tree,\n",
    "                # and update the weight to be the current weight times the proportion of samples in the left child node\n",
    "                left_subtree = traverse_tree(left_child, weight * tree.proportion_of_samples_in_left_child(node_i))\n",
    "                \n",
    "                # TODO: traverse the right sub-tree,\n",
    "                # and update the weight to be the current weight times the proportion of samples in the left child node\n",
    "                right_subtree = traverse_tree(right_child, weight * tree.proportion_of_samples_in_right_child(node_i))\n",
    "                \n",
    "                # TODO: return the sum of both sub-trees\n",
    "                return left_subtree + right_subtree\n",
    "    \n",
    "    # start traversing the tree\n",
    "    return traverse_tree(starting_node,starting_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try out the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_values = np.array([1,1,1])\n",
    "S = set([2]) # if you input only feature 2, expect 0.25\n",
    "f_given_S(tree_wrap, S, sample_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try empty feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = set([]) #for empty set, expect output to be 0.25\n",
    "\n",
    "f_given_S(tree_wrap, S, sample_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate weight on marginal contribution\n",
    "\n",
    "We'll calculate the weight placed on the marginal contribution of the feature:\n",
    "$ \\frac{|S|! (M - |S| -1 )!}{M!}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import factorial\n",
    "\n",
    "def weight_on_marginal_contribution(size_S, M):\n",
    "    \"\"\"\n",
    "    size_S: number of features in set S\n",
    "    M: numer of total features\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return factorial(size_S) * factorial(M - size_S - 1) / factorial(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate marginal contribution for a single feature \n",
    "\n",
    "$ f(S \\cup i) - f(S)$  \n",
    "\n",
    "Fill in the function that takes in the custom Tree object, a sample data point, a list containing the set of features in set S (excluding feature \"i\"), and also the feature for which we want to calculate the marginal contribution.  Keep in mind that set S excludes feature \"i\".\n",
    "\n",
    "**Hint:** The python `set` class has the member function `.add`.  \n",
    "Note that you may need to use the `.copy` function as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marginal_contribution_of_feature(tree, x, S, feature_i):\n",
    "    \"\"\"\n",
    "    tree: the custom Tree object that wraps the sklearn tree_ object\n",
    "    x: a sample observation that contains all features\n",
    "    S: a list of integers, specifying the features in subset S, excluding feature i.\n",
    "    feature_i: an integer specifying the feature for which we're calculating the marginal contribution.\n",
    "    \"\"\"\n",
    "    # TODO: create the union of S and i\n",
    "    S_union_i = S.copy()\n",
    "    S_union_i.add(feature_i)\n",
    "    \n",
    "    # TODO: return the difference in prediction with feature \"i\" and without feature \"i\"\n",
    "    return f_given_S(tree, S_union_i, x) - f_given_S(tree, S, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try it out\n",
    "\n",
    "We'll try out the `marginal_contribution_of_feature` function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_i = 0  # index of feature for which we want to calculate its marginal contribution\n",
    "S = set([1])  # set that excludes feature i\n",
    "x = X[0]  # grab one data point to calculate marginal contribution on\n",
    "\n",
    "marginal_contribution_of_feature(tree_wrap,x,S,feature_i) # we expect 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The marginal contribution of feature 0 is 0.5.  This means that the prediction of the model when feature 0 is present is 0.5 greater than the model's prediction when it only has feature 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate all subsets\n",
    "\n",
    "Fill in a function that generates all possible subsets S.  \n",
    "We'll use `itertools.combinations`, which takes in a list, and also the size of each subset.  It returns an iterable object that contains tuples of all the combinations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(1, 3)\n",
      "(1, 4)\n",
      "(2, 3)\n",
      "(2, 4)\n",
      "(3, 4)\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# try out the combinations function\n",
    "tmp_combo = combinations([1,2,3,4], 2)\n",
    "for subset in tmp_combo:\n",
    "    print(subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fill in the function generate_all_subsets\n",
    "\n",
    "Keep in mind that since the iterable returned by `combinations` holds tuples, we can create sets out of the tuples by using `set(the_tuple_object)`.  We'll store the S sets as `set` types, since we defined the `f_given_S` function to take S as a type `set`.\n",
    "\n",
    "Remember to also include the empty set.  We can do this with `set([None])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_subsets(all_features):\n",
    "    \"\"\"\n",
    "    all_features: set of integers representing all the features\n",
    "    \"\"\"\n",
    "    sets_l = []\n",
    "    for size in range(1, len(all_features) + 1):\n",
    "        # TODO: create a combinations iterable\n",
    "        combo = combinations(all_features, size)\n",
    "\n",
    "        # TODO: loop thru the combo iterable and append sets to the sets_l list\n",
    "        for subset in combo:\n",
    "            sets_l.append(set(subset))\n",
    "            \n",
    "    # TODO: also include the empty set\n",
    "    sets_l.append(set([None]))\n",
    "    \n",
    "    return sets_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try out the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "[{0}, {1}, {2}, {0, 1}, {0, 2}, {1, 2}, {0, 1, 2}, {None}]"
      ],
      "text/plain": [
       "[{0}, {1}, {2}, {0, 1}, {0, 2}, {1, 2}, {0, 1, 2}, {None}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features = set([0, 1, 2])\n",
    "generate_all_subsets(all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate shapley value for one feature\n",
    "\n",
    "Implement a function that calculates the shapley value for a single feature, by iterating across all subsets S.\n",
    "\n",
    "$ \\phi_{i} = \\sum_{S \\subseteq M \\setminus i} \\frac{|S|! (|M| - |S| -1 )!}{|M|!} [f(S \\cup i) - f(S)]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_feature_i(tree, x, feature_i):\n",
    "    \"\"\"\n",
    "    tree: the custom Tree object that wraps the tree_ from sklearn.\n",
    "    x: a sample data point\n",
    "    i: the feature for which we want to calculate its shapley value.\n",
    "    \"\"\"\n",
    "    all_features = set(np.arange(0, x.shape[0]))\n",
    "    all_features_minus_i = all_features.copy()\n",
    "    all_features_minus_i.remove(feature_i)  # remove feature \"i\"\n",
    "\n",
    "    # TODO: generate all subsets S\n",
    "    S_list = generate_all_subsets(all_features_minus_i)\n",
    "    phi = 0\n",
    "    num_features_total = len(all_features)\n",
    "    \n",
    "    # iterate thru S_list\n",
    "    for S in S_list:\n",
    "        # TODO: calculate the number of features stored in S\n",
    "        # Handle the special case where S contains None, because 'None' shouldn't count as a feature\n",
    "        if None in S:\n",
    "            size_S = len(S) - 1\n",
    "        else:\n",
    "            size_S = len(S)\n",
    "            \n",
    "        # TODO: increment phi by the weight on the marginal contribution * marginal contribution of \"i\"\n",
    "        phi += weight_on_marginal_contribution(size_S, num_features_total) * \\\n",
    "               marginal_contribution_of_feature(tree, x, S, feature_i)\n",
    "    \n",
    "    return np.round(phi, decimals=3)\n",
    "    #return phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try out the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.375"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = X[0]\n",
    "shap_feature_i(tree_wrap, x, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate feature importance of all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_tree_explainer(tree_wrap, x):\n",
    "    shap_l = []\n",
    "    for i, v in enumerate(x):\n",
    "        shap_l.append(shap_feature_i(tree_wrap, x,i))\n",
    "    \n",
    "    return np.array(shap_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## take an sklearn tree model and calculate feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_tree_model_explainer(tree_model, x):\n",
    "    sklearn_tree = tree_model.tree_\n",
    "    tree_wrap = Tree(tree_model.tree_)\n",
    "    \n",
    "    return shap_tree_explainer(tree_wrap,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additive Feature Attribution\n",
    "\n",
    "Additive feature attribution methods are simple models that are used to explain complex models.  You can see the formula in the same paper on page 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./tree_shap_images/tree_shap_img_13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### explanation\n",
    "\n",
    "Think of our tree model as the complex model that we wish to explain with a simple, linear model.  The above formula is saying that we can take a single data point with the three features, and the complex model makes a prediction.  We can divide up that prediction among the three features, based on how important those features are to the complex model's prediction, and also based on whether the feature values push the prediction in the positive or negative direction.\n",
    "\n",
    "This is related to the ideas of coalition game theory.  Imagine a team of basketball players scores 100 points in a game. We are trying to attribute part of the final score to each member of the team, based on their contributions, or \"importance.\"\n",
    "\n",
    "When the contributions of each feature are added up to equal the complex model's prediction, this linear combination of contributions is the simple linear model that is being used to explain the complex model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### example\n",
    "\n",
    "Let's say that we've trained a complex model on 3 features.  If it's given no inputs to make a prediction, then its prediction would be the equal weighted average of all its training samples.  \n",
    "Let's say that equal weighted average of the training labels is **100**.  In other words, if a model is given no features and asked to make a prediction, it would predict 100, which is the expected value based on the training labels.\n",
    "\n",
    "Now, let's say we give the complex model a single sample observation, with all three features, and the complex model gives a prediction of **200**.  \n",
    "\n",
    "The additive feature attribution model may assign feature importances to the three features like this:  \n",
    "feature 0: +50  \n",
    "feature 1: +90  \n",
    "feature 2: -40  \n",
    "\n",
    "So this is saying that feature 0 pushed the complex model's prediction up by 50, feature 1 pushed the complex model's prediction up by 90, and feature 2 pushed the model's prediction down by 40.  The end result was to go from the expected value of 100 to the prediction of 200.\n",
    "\n",
    "The shapley values that we just calculated are these values that push the model's prediction from the average of the training labels to the model's final prediction.  When we add up the shapley values for all the features, they add up to the model's prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare this implementation with shap library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my shap function: [-0.125 -0.125  0.   ]\n",
      "shap library:     [-0.125 -0.125  0.   ]\n",
      "expected value (average of labels in y) 0.25\n",
      "sum of shapley values for all features: -0.25\n",
      "model prediction [0.]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0, 0, 0])\n",
    "shap_values = shap_tree_model_explainer(model,x)\n",
    "expected_value = np.mean(y)\n",
    "print(f\"my shap function: {shap_values}\")\n",
    "print(f\"shap library:     {shap.TreeExplainer(model).shap_values(x)}\")\n",
    "print(f\"expected value (average of labels in y) {expected_value}\")\n",
    "print(f\"sum of shapley values for all features: {np.sum(shap_values)}\")\n",
    "print(f\"model prediction {model.predict(x.reshape(1,-1))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### quiz\n",
    "\n",
    "How do you interpret the shapley values of each feature when features 0,1 and 2 are all 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### answer\n",
    "\n",
    "We can think of how much each feature pushes the model towards its final prediction of 0, when features 0, 1 and 2 are all 0. Feature 0 and 1 are equally important, so they each contribute half towards pushing the prediction from 0.25 to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my shap function: [ 0.125 -0.375  0.   ]\n",
      "shap library:     [ 0.125 -0.375  0.   ]\n",
      "expected value (average of labels in y) 0.25\n",
      "sum of shapley values for all features: -0.25\n",
      "model prediction [0.]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 0, 0])\n",
    "shap_values = shap_tree_model_explainer(model,x)\n",
    "print(f\"my shap function: {shap_values}\")\n",
    "print(f\"shap library:     {shap.TreeExplainer(model).shap_values(x)}\")\n",
    "print(f\"expected value (average of labels in y) {expected_value}\")\n",
    "print(f\"sum of shapley values for all features: {np.sum(shap_values)}\")\n",
    "print(f\"model prediction {model.predict(x.reshape(1,-1))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### quiz\n",
    "\n",
    "How do you interpret the shapley values of each feature when feature 0 is 1 and the other features are 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### answer\n",
    "\n",
    "Feature 0 has a value of 1.  So it is pushing the prediction in the positive direction.  Feature 1 has a value of 0, so it's pushing the prediction in the negative direction.  When their contributions are combined, they add up to -0.25, which pushes the prediction from the expected value of 0.25 down to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my shap function: [0.375 0.375 0.   ]\n",
      "shap library:     [0.375 0.375 0.   ]\n",
      "expected value (average of labels in y) 0.25\n",
      "sum of shapley values for all features: 0.75\n",
      "model prediction [1.]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 1, 0])\n",
    "shap_values = shap_tree_model_explainer(model,x)\n",
    "print(f\"my shap function: {shap_values}\")\n",
    "print(f\"shap library:     {shap.TreeExplainer(model).shap_values(x)}\")\n",
    "print(f\"expected value (average of labels in y) {expected_value}\")\n",
    "print(f\"sum of shapley values for all features: {np.sum(shap_values)}\")\n",
    "print(f\"model prediction {model.predict(x.reshape(1,-1))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### quiz\n",
    "\n",
    "How do we interpret the shapley values when feature 0 and 1 are both 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### answer\n",
    "\n",
    "When both features 0 and 1 are 1, the prediction is 1.  So the features are pushing the prediction from the expected value of 0.25 to 1, a distance of 0.75.  This distance is evenly split between features 0 and 1, so each feature has a shapley values of 0.375 each.  Also notice that feature 0 has a shapley value of 0 in all of our examples, because it doesn't contribute to the model's prediction in either direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "[solution notebook](tree_shap_solution.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
