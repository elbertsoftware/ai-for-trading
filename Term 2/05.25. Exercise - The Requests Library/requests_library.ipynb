{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Requests Library\n",
    "\n",
    "Now that we know how to use BeautifulSoup to get data from HTML files, let's see how we can scrape data from a real website. Unfortunately, Beautifulsoup can't access websites directly. Therefore, in order to access websites, we will use Python's `requests` library. The `requests` library allows us to send web requests and get a website's HTML data. Once the `requests` library gets us the HTML data, we can use Beautifulsoup, just as we did before, to extract the data we want. So let's see an example.\n",
    "\n",
    "In the code below we will use the `requests` library and BeautifulSoup to get Tesla's `production and sales by quarter` data from a `html table` the following Wikipedia [webpage](https://en.wikipedia.org/wiki/Tesla,_Inc.). This table corresponds to Tesla's production and sales figures since Q1 2013. We will start by importing the `requests` library by using:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "```\n",
    "\n",
    "We will then use the `requests.get(website)` function to get the source code from our `wikipage`. The `requests.get()` function returns a `Response` object that we will save in the variable `r`. We can get the HTML data we need from this object by using the `.text` method, as shown below. Finally, we'll convert and display the extracted html table into Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Response object\n",
    "r = requests.get('https://en.wikipedia.org/wiki/Tesla,_Inc.')\n",
    "\n",
    "# Get HTML data\n",
    "html_data = r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.text` method returns a string, therefore, `html_data` is a string containing the HTML data from our website. Notice, that since `html_data` is a string it can be passed to the BeautifulSoup constructor, and we will do this next, but for now, let's print the `html_data` string to see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the HTML data\n",
    "print(html_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, `html_data` indeed contains the HTML data of our website. Notice, that since we are dealing with a real website this time, the HTML file is very long. \n",
    "\n",
    "Now that we have the HTML data from our website, we are ready to use BeautifulSoup just as we did before. The only difference is that this time, instead of passing an open filehandle to the BeautifulSoup constructor, we will pass the `html_data` string. So let's pass `html_data` to the BeautifulSoup constructor to get a BeautifulSoup object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a BeautifulSoup Object\n",
    "page_content = BeautifulSoup(html_data, 'html.parser')\n",
    "\n",
    "# Print the BeautifulSoup Object\n",
    "print(page_content.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a BeautifulSoup object, we can get our sales and production data. To do this, we need to know which tags contain our table data. In order to figure this out, we need to inspect our webpage using our web browser. For this example we will use the Chrome web browser but all web browsers have the same kind of functionality. We begin by going to our wikipage: https://en.wikipedia.org/wiki/Tesla,_Inc.\n",
    "\n",
    "Next, we hover our mouse over around the [target html table](https://en.wikipedia.org/wiki/Tesla,_Inc.#Production_and_sales_by_quarter). As an example, we will hover over the `table header` row. Next, we right-click on the header title for `Quarter` and a dropdown menu will appear. From this menu we will choose **Inspect**. Once we click on **Inspect** we will see the HTML source code of our webpage appear on the right, as shown in the figure below:\n",
    "\n",
    "<br>\n",
    "<figure>\n",
    "  <img src = \"./wikitable.png\" width = \"100%\" style = \"border: thin silver solid; padding: 10px\">\n",
    "</figure> \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we inspect the html source code on the right panel, we will see that the table header and rows are all contained within the same `<table>` tag. Therefore, we can use the above information to extract the column title from table header and detail data in the rows. In the code below we use BeautifulSoup's `find()` method to find the target `<table>` tag that has `class=\"wikitable\"`.\n",
    "\n",
    "<br>\n",
    "<figure>\n",
    "  <img src = \"./table-tag.png\" width = \"50%\" style = \"border: thin silver solid; padding: 10px\">\n",
    "</figure> \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A complete `html table` can be structured in the following way:\n",
    "```\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Month</th>\n",
    "      <th>Sales</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  \n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>January</td>\n",
    "      <td>$100</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>February</td>\n",
    "      <td>$80</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "  \n",
    "  <tfoot>\n",
    "    <tr>\n",
    "      <td>Sum</td>\n",
    "      <td>$180</td>\n",
    "    </tr>\n",
    "  </tfoot>\n",
    "</table>\n",
    "```\n",
    "\n",
    "But notice that the html table in our wikipage does not have `<thead>` and `<tfoot>` tags. Let's create a BeautifulSoup's object and name it as `wikitable`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikitable = page_content.find('table', {'class': 'wikitable'})\n",
    "wikitable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have extracted the `<table>` tag that have `class=\"wikitable\"` as an object, we can use BeautifulSoup's built-in functions to extract the human-readable text inside this table. Let's grab the information inside `<tbody>` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikitable.tbody"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the earlier cell, we use `find()` function to get the html table. Now, we are going to use `findAll()` function to grab all the defined tags inside the BeautifulSoup object. In this case, we want to extract every row in the `<tbody>` tag and we can do so by using `findAll()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikitable.tbody.findAll('tr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the html source code above, the first row consists of the column name and followed by the actual data in the subsequent rows. Let's grab the `column names` from the first row and all the `<th>` tags inside the row.\n",
    "\n",
    "Notice the row index of `[0]` after `findAll('tr')` to get the first row. Then, we can chain the second `findAll()` function to get all the `<th>` tags inside this row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikicolumns = wikitable.tbody.findAll('tr')[0].findAll('th')\n",
    "wikicolumns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store the column names in a Python object, called `df_columns`, so we can use it to build Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns = []\n",
    "\n",
    "for column in wikicolumns:\n",
    "    # remove <br/> inside <th> text, such as `<th>Total<br/>production</th>`\n",
    "    text = column.get_text(strip=True, separator=\" \")\n",
    "    # append the text into df_columns\n",
    "    df_columns.append(text)\n",
    "\n",
    "print(np.array(df_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we hava stored the column names, now we want to iterate the remaining rows, consisting the real data in this table. We can use Python `for loop` function to iterate from the second row onward. To do so, we need to set the starting index as follows: `wikitable.tbody.findAll('tr')[1:]`. Let's store our dataset in Python object, called `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = []\n",
    "\n",
    "for row in wikitable.tbody.findAll('tr')[1:]:\n",
    "    row_data = []\n",
    "    for td in row.findAll('td'):\n",
    "        text = td.get_text(strip=True, separator=\" \")\n",
    "        row_data.append(text)\n",
    "    df_data.append(np.array(row_data))\n",
    "\n",
    "# print the first 10 data rows\n",
    "print(df_data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have grabbed the column names and data. But we want to present the data in human-readable structure. We can use [Pandas DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) library. Pandas DataFrame is similar to Excel spreadsheet and Google Sheet. This library provides a convenient data structure to manipulate and present data with Python.\n",
    "\n",
    "Let's create a Panda DataFrame object, called `dataframe`, so we can present our data in a spreadsheet structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(data=df_data, columns=df_columns)\n",
    "dataframe.set_index('Quarter', inplace=True)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job! We have extracted Tesla's production and sales data from an `html table` in a Wikipage and converted the data into Python and Pandas DataFrame. It's now your time to practice with `requests` and `BeautifulSoup` libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Get Amazon financial data from Wikipage\n",
    "\n",
    "URL links:\n",
    "- Wikipage: https://en.wikipedia.org/wiki/Amazon_(company) <br/>\n",
    "- Financial data: https://en.wikipedia.org/wiki/Amazon_(company)#Finances\n",
    "\n",
    "Tasks: <br/>\n",
    "Start by importing the `BeautifulSoup` and `requests` libraries. Then use the `requests.get()` function with the appropriate `params` to get our website's HTML data. Then create a BeautifulSoup Object named `page_content` using our website's HTML data and the `html_parser` parser. Then use the `find()` method to find the `<table>` tag. Then, get the table column names. Finally, create a loop that prints all the countries and population from `<tbody>` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a Response object\n",
    "r = \n",
    "\n",
    "# Get HTML data\n",
    "html_data = \n",
    "\n",
    "# Create a BeautifulSoup Object\n",
    "page_content = \n",
    "\n",
    "# Find financial table\n",
    "wikitable = \n",
    "\n",
    "# Find all column titles\n",
    "wikicolumns = \n",
    "\n",
    "# Loop through column titles and store into Python array\n",
    "df_columns = []\n",
    "\n",
    "# Loop through the data rows and store into Python array\n",
    "df_data = []\n",
    "\n",
    "# Print financial data in DataFrame format and set `Year` as index\n",
    "dataframe = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XML\n",
    "\n",
    "Throughout these lessons we have used HTML files and BeautifulSoup's `html_parser` to show you how to scrape data. We should note that the exact same techniques can be applied to XML files. The only difference is that you will have to use an XML parser in the BeautifulSoup constructor. For example, in order to parse a document as XML, you can use `lxml`’s XML parser by passing in `xml` as the second argument to the BeautifulSoup constructor:\n",
    "\n",
    "```python\n",
    "page_content = BeautifulSoup(xml_file, 'xml')\n",
    "```\n",
    "\n",
    "The above statement will parse the given `xml_file` as XML using the `xml` parser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Remarks\n",
    "\n",
    "So now you should know how to scrape data from websites using the `requests` and `BeautifulSoup` libraries. We should note, that you should be careful when scrapping websites not to overwhelm a website's server. This can happen if you write computer programs that send out a lot of requests very quickly. Doing this, will overwhelm the server and probably cause it to get stuck. This is obviously very bad, so avoid making tons of web requests in a short amount of time. In fact, some servers monitor if you are making too many requests and block you, if you are doing so. So keep this in mind when you are writing computer programs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution\n",
    "\n",
    "[Solution notebook](requests_library_solution.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "147px",
    "width": "322px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
